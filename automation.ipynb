{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLx2WBXT9_r7",
        "outputId": "72093f61-26f0-4af6-85c5-d0b76cd61508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.5.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.5.0-py3-none-any.whl (329 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m329.6/329.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf, faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.2 pypdf-6.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install transformers sentence-transformers faiss-cpu pypdf nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U0kLHfQ-CNt",
        "outputId": "7c928689-7a94-46c6-c1f1-f529a8daafab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import nltk\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "from pypdf import PdfReader\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ffihx_5U-CQV"
      },
      "outputs": [],
      "source": [
        "nltk.download(\"punkt\")\n",
        "from nltk.tokenize import sent_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c_wQgIjF-CS9"
      },
      "outputs": [],
      "source": [
        "def load_document(file_path):\n",
        "    if file_path.endswith(\".pdf\"):\n",
        "        reader = PdfReader(file_path)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "\n",
        "    elif file_path.endswith(\".txt\"):\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return f.read()\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzeVswTm-CVl"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, max_sentences=5):\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(sentences), max_sentences):\n",
        "        chunk = \" \".join(sentences[i:i+max_sentences])\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOjJL40r-CX9"
      },
      "outputs": [],
      "source": [
        "# Summarization LLM\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=\"facebook/bart-large-cnn\"\n",
        ")\n",
        "\n",
        "# Embedding model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "639PkKYO-Cad"
      },
      "outputs": [],
      "source": [
        "def summarize_document(text):\n",
        "    summary = summarizer(\n",
        "        text[:1024],\n",
        "        max_length=150,\n",
        "        min_length=50,\n",
        "        do_sample=False\n",
        "    )\n",
        "    return summary[0][\"summary_text\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHeRpT9d-CcS"
      },
      "outputs": [],
      "source": [
        "def generate_tags(summary):\n",
        "    keywords = []\n",
        "    for word in summary.lower().split():\n",
        "        if len(word) > 6 and word.isalpha():\n",
        "            keywords.append(word)\n",
        "\n",
        "    return list(set(keywords))[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bE7NZraV-Cdv"
      },
      "outputs": [],
      "source": [
        "def build_vector_store(chunks):\n",
        "    embeddings = embedding_model.encode(chunks)\n",
        "    dimension = embeddings.shape[1]\n",
        "\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(np.array(embeddings))\n",
        "\n",
        "    return index, embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCV3q3RL-Cgq"
      },
      "outputs": [],
      "source": [
        "def retrieve_chunks(question, chunks, index, top_k=3):\n",
        "    query_embedding = embedding_model.encode([question])\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    return [chunks[i] for i in indices[0]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvuTgfnj-CiW"
      },
      "outputs": [],
      "source": [
        "def answer_question(question, chunks, index):\n",
        "    retrieved = retrieve_chunks(question, chunks, index)\n",
        "\n",
        "    if not retrieved:\n",
        "        return \"SORRY I DON'T KNOW\"\n",
        "\n",
        "    context = \" \".join(retrieved)\n",
        "\n",
        "    prompt = (\n",
        "        \"Answer ONLY from the context below.\\n\"\n",
        "        \"If the answer is not in the context, say 'SORRY I DON'T KNOW'.\\n\\n\"\n",
        "        f\"Context:\\n{context}\\n\\nQuestion:\\n{question}\"\n",
        "    )\n",
        "\n",
        "    response = summarizer(\n",
        "        prompt[:1024],\n",
        "        max_length=120,\n",
        "        min_length=40,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "    return response[0][\"summary_text\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_document(file_path):\n",
        "    print(\"üìÑ Loading document...\")\n",
        "    text = load_document(file_path)\n",
        "\n",
        "    print(\"‚úÇÔ∏è Chunking text...\")\n",
        "    chunks = chunk_text(text)\n",
        "\n",
        "    print(\"üß† Summarizing document...\")\n",
        "    summary = summarize_document(text)\n",
        "\n",
        "    print(\"üè∑Ô∏è Generating tags...\")\n",
        "    tags = generate_tags(summary)\n",
        "\n",
        "    print(\"üì¶ Creating embeddings...\")\n",
        "    index, _ = build_vector_store(chunks)\n",
        "\n",
        "    print(\"\\n‚úÖ Document Processed Successfully\\n\")\n",
        "    print(\"SUMMARY:\\n\", summary)\n",
        "    print(\"\\nTAGS:\", tags)\n",
        "\n",
        "    return chunks, index\n"
      ],
      "metadata": {
        "id": "vPT6XxUSAgoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    file_path = \"documents/sample.pdf\"\n",
        "\n",
        "    chunks, index = process_document(file_path)\n",
        "\n",
        "    print(\"\\nüí¨ Ask questions (type 'exit' to quit)\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"User: \")\n",
        "\n",
        "        if query.lower() == \"exit\":\n",
        "            break\n",
        "\n",
        "        answer = answer_question(query, chunks, index)\n",
        "        print(\"AI:\", answer, \"\\n\")\n"
      ],
      "metadata": {
        "id": "F2LU2d2AAgql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HIzI1eDdAgtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GB7HJRR8Agvb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}